---
#==================================================================================================================================
# Variables to override the playbook defaults and specify which tasks will run in the install_ecp_control_plane.yml playbook
# NOTE: By default, the following (5) tasks are run: install_controller, add_gateways, add_ha_hosts, configure_ha and add_host_tags
# You can override this behavior by uncommenting the appropriate line(s) to select which task(s) should not run
#==================================================================================================================================
#install_controller: no
#add_gateways: no
#add_ha_hosts: no
#configure_ha: no
#add_host_tags: no

#==================================================================================================================================
# These variables govern the execution of optional tasks. These tasks are not run by default in install_ecp_control_plane playbook
# You can override this behavior by uncommenting the appropriate line(s) to select which task(s) should run
#==================================================================================================================================
#configure_gateway_ports: yes
#configure_airgap: yes
#configure_notification: yes
# NOTE: these tasks are only needed with ECP 5.1
#apply_ecp_patches: yes
#apply_ntp_fix_primary: yes
#apply_ntp_fix_shadow: yes

platform:
  name: RTLAB
  type: onprem
  api_scheme: http    # Setting for API calls - usually http regardless
  controller: 
    config:
      bd_domain: hpelab
      bd_prefix: ecp
      int_start_ip: 172.20.0.2
      int_end_ip: 172.20.255.254
      int_gw_ip: 172.20.0.1
      int_nw_mask: 16
      # ssl_cert_file: /var/lib/awx/mip-bdcs-vm19.mip.storage.hpecorp.net/cert.pem
      # ssl_cert_key_file: /var/lib/awx/mip-bdcs-vm19.mip.storage.hpecorp.net/key.pem
    disks:      
      node_disks:
        - /dev/sdb
      hdfs_disks:
        - /dev/sdc
    no_tenant_storage: false
    host: mip-bdcs-vm31.mip.storage.hpecorp.net
  gateways:
    config:
      gateway_set_hostname: hpecp-lab-gw.mip.storage.hpecorp.net
      # ssl_cert_file: cert.pem
      # ssl_key_file: key.pem
      port_mapping_start: 10000
      port_mapping_end: 11000
    hosts:
      - hpecp-lab-gw.mip.storage.hpecorp.net    # Gateway 1
  install:
    install_type: non-agent-based-install
    install_as_root: true
    ###############################################
    # Installer and Prechecks file info for ECP 5.3
    ###############################################  
    bin_url: "http://16.143.20.46:8080/rock/golden/5.3-GA/3031/hpe-cp-rhel-release-5.3-3031.bin"
    installer_file: "hpe-cp-rhel-release-5.3-3031.bin"
    prechecks_file: "hpe-cp-rhel-prechecks-5.3.bin"
    #-----------------------------------------
    # Installer and Prechecks file for ECP 5.2  
    #-----------------------------------------
    # bin_url: "https://bluedata-releases.s3.amazonaws.com/5.2/hpe-cp-rhel-release-5.2-3020.bin"  
    # installer_file: "hpe-cp-rhel-release-5.2-3020.bin"
    # prechecks_file: "hpe-cp-rhel-prechecks-5.2.bin"
    #-----------------------------------------
    # Installer and Prechecks file for ECP 5.1  
    #-----------------------------------------
    # bin_url: "https://bluedata-releases.s3.amazonaws.com/5.1/hpe-cp-rhel-release-5.1-3011.bin"
    # installer_file: hpe-cp-rhel-release-5.1-3011
    # prechecks_file: "hpe-cp-rhel-prechecks-5.2.bin"
    no_tenant_isolation: false
  proxy: http://web-proxy.corp.hpecorp.net:8080 
  rest_protocol: http
  validate_certs: no
  tools_dir: "/root/tools"         # place to store the kits like kubectl

ecp_airgap:
  container_repo_url: 16.143.22.165:8080/hpecp
  # container_repo_username:
  # container_repo_password:
  # container_repo_secure_flag:
  # container_repo_cert:
  # yum_repo_gpg_key:
  # yum_rpm_gpg_key:
  # yum_repo_url: http://16.143.20.46:8080/scratch/qa/repos/kubernetes/   # Unsupported - bad RPM
  yum_repo_url: http://bd-repos1.mip.storage.hpecorp.net/kubernetes/
  
ecp_credentials:
  site_admin_id: admin 
  site_admin_password: admin123  # Used by epicctl to set the default password for the site admin account during installation
  ssh:
    # access_type: ssh_key_access
    access_type: password_access
    # keypair_file: /home/stack/.ssh/prod_ecp_rsa
    # keypair_name: prod_ecp_rsa
    # key_passphrase: bubAIdT^k2l}
    password: admin123
    username: root
    usergroup: root

ecp_epic:
  ha_nodes:
    disks:
      node_disks:
        - /dev/sdb
        # - /dev/sdc
      hdfs_disks:
        - /dev/sdc
    hosts:
      - mip-bdcs-vm33.mip.storage.hpecorp.net    # 16.143.22.158 Shadow
      - mip-bdcs-vm35.mip.storage.hpecorp.net    # 16.143.22.160 Arbiter
    no_tenant_storage: true
    shadow_host: mip-bdcs-vm33.mip.storage.hpecorp.net   # 16.143.22.158 Shadow
    arbiter_host: mip-bdcs-vm35.mip.storage.hpecorp.net  # 16.143.22.160 Arbiter
  worker_nodes:
    disks:
      node_disks:
        - /dev/sdb
        # - /dev/sdc
      hdfs_disks:
        - /dev/sdc
    hosts:
      - mip-bdcs-vm39.mip.storage.hpecorp.net    # 16.143.22.164 MLOps EPIC Worker
      # - mip-bdcs-vm31.mip.storage.hpecorp.net    # 16.143.22.156 Arbiter
    no_tenant_storage: true

  #################################################################################################
  # The ecp_epic.hosts_to_delete variable is ONLY used to delete EPIC hosts from the ECP platform #
  # This list can be either IPs or FQDNs                                                          #
  #         - OR -                                                                                #
  # the hosts to delete can be specified using a status.                                          #
  # NOTE: ALL EPIC hosts with that status will be DELETED from the platform.                      #
  #################################################################################################
  hosts_to_delete:
    list:
      - mip-bdcs-vm35.mip.storage.hpecorp.net   
      - mip-bdcs-vm37.mip.storage.hpecorp.net    
      # - mip-bdcs-vm39.mip.storage.hpecorp.net
    # status: ready      

ecp_k8s:
  master_nodes:
    disks:
      ephemeral_disks:
        - /dev/sdb
        - /dev/sdc
      persistent_disks:
        # - /dev/sdc
    hosts:
      #- hpecp-splunk.mip.storage.hpecorp.net      # 16.143.22.159  K8s Compute Master1
      - 16.143.22.159                             # hpecp-splunk.mip.storage.hpecorp.net  K8s Compute Master1
    no_tenant_storage: true
  # ingress_nodes:
  #   disks:
  #     ephemeral_disks:
  #       - /dev/sdb
  #       - /dev/sdc
  #     persistent_disks:
  #       - /dev/sdc
  #   hosts:
  #     - hpecp-splunk.mip.storage.hpecorp.net      # 16.143.22.159  K8s Ingress 1
  #     - mip-bdcs-vm35.mip.storage.hpecorp.net     # 16.143.22.160  K8s Ingress 2
  #   no_tenant_storage: true
  # cp_nodes:
  #   disks:
  #     ephemeral_disks:
  #       - /dev/sdb
  #       - /dev/sdc
  #     persistent_disks:
  #       - /dev/sdc
  #   hosts:
  #     - hpecp-splunk-master.mip.storage.hpecorp.net   # 16.143.22.161  K8s CP Worker 1
  #     - mip-bdcs-vm37.mip.storage.hpecorp.net         # 16.143.22.162  K8s CP Worker 2
  #   no_tenant_storage: true
  worker_nodes:
    disks:
      ephemeral_disks:
        - /dev/sdb
        - /dev/sdc
      persistent_disks:
        # - /dev/sdc
    hosts:
      - mip-bdcs-vm35.mip.storage.hpecorp.net         # 16.143.22.160  K8s Compute Worker 1
      # - hpecp-splunk-master.mip.storage.hpecorp.net   # 16.143.22.161  K8s Compute Worker 2
      - 16.143.22.161                                 # hpecp-splunk-master.mip.storage.hpecorp.net  K8s Compute Worker 2
    no_tenant_storage: true
  ###############################################################################################
  # The ecp_k8s.hosts_to_delete variable is ONLY used to delete K8S hosts from the ECP platform #
  # This list can be either IPs or FQDNs                                                        #
  #         - OR -                                                                              #
  # the hosts to delete can be specified using a status.                                        #
  # NOTE: ALL K8s hosts with that status will be DELETED from the platform.                     #
  ###############################################################################################
  hosts_to_delete:
    list:
      - mip-bdcs-vm35.mip.storage.hpecorp.net   
      - mip-bdcs-vm37.mip.storage.hpecorp.net    
      # - mip-bdcs-vm39.mip.storage.hpecorp.net
    # status: ready      

  ############################################################################################
  # OPTIONAL: the ecp_k8s.df_master_nodes and ecp_k8s.df_worker_nodes variables are used for #
  # configuring K8s data fabric hosts for use by a K8s data fabric cluster.                  #
  # NOTE: a minimum of (3) df master nodes and (5) df worker nodes are required.             #
  ############################################################################################
  df_master_nodes:
    disks:
      ephemeral_disks:
        - /dev/sdb
        # - /dev/sdc
      persistent_disks:
        - /dev/sdc
    hosts:
      - mip-bdcs-vm37.mip.storage.hpecorp.net     # 16.143.22.162  K8s Data Fabric Master1
      - mip-bdcs-vm38.mip.storage.hpecorp.net     # 16.143.22.163  K8s Data Fabric Master2
      - mip-bdcs-vm39.mip.storage.hpecorp.net     # 16.143.22.164  K8s Data Fabric Master3
    no_tenant_storage: false
  df_worker_nodes:
    disks:
      ephemeral_disks:
        - /dev/sdb
        # - /dev/sdc
      persistent_disks:
        - /dev/sdc
    hosts:
      - mip-bdcs-vm52.mip.storage.hpecorp.net         # 16.143.22.185  K8s Data Fabric Worker 1
      - mip-bdcs-vm53.mip.storage.hpecorp.net         # 16.143.22.186  K8s Data Fabric Worker 2
      - mip-bdcs-vm54.mip.storage.hpecorp.net         # 16.143.22.187  K8s Data Fabric Worker 3
      - mip-bdcs-vm55.mip.storage.hpecorp.net         # 16.143.22.188  K8s Data Fabric Worker 4
      - mip-bdcs-vm56.mip.storage.hpecorp.net         # 16.143.22.189  K8s Data Fabric Worker 5
    no_tenant_storage: false

  cluster:
    name: test
    description: Test K8s 1.20.2 cluster

    #####################
    # I M P O R T A N T #
    #####################
    # In order for ECP 5.2 to support K8s version 1.19, the following playbooks must be run BEFORE creating the K8s cluster
    # apply_k8s_manifest_patch.yml
    # update_k8s_manifest.yml
    k8s_version: 1.20.2
    id: 0
    pod_network_range: "10.192.0.0/12"
    service_network_range: "10.96.0.0/12"
    pod_dns_domain: "cluster.local"
    cert_data:
      root_ca_cert: 
      root_ca_key:
      front_proxy_ca_cert:
      front_proxy_ca_key:
      etcd_ca_cert:
      etcd_ca_key:
    ext_identity_server:
      type: LDAP
      ip: 
      port: 0
      auth_service_location_host:
      auth_service_location_port: 0
      timeout_ms: 1000
      reorder_after_failover: false
      user_attribute:
      bind_type: search_bind
      base_dn:
      security_protocol: none
      cacert_filename:
      nt_domain:
      bind_dn:
      bind_pwd:
      verify_peer: true
    external_groups: cn=Common_Name,ou=Organizational_Unit,dc=Domain_Component

    ########################################################
    # Variables for Adding/Removing nodes from K8s cluster #
    ########################################################
    target_cluster_name: test
    nodes_to_add:
      - 16.143.22.160                   # K8s Ingress 2
      - 16.143.22.162                   # K8s CP Worker 2
    # NOTES:
    #  1. This variable is only used by the remove_nodes_from_k8s_cluster playbook
    #  2. The list of nodes to remove can be designated using IP addresses or hostnames
    #  3. Alternatively, a node status can be used to designate the nodes to remove
    #     HOWEVER, either the list or the status must be empty. You can't use both 
    nodes_to_remove:
      list: 
        - 16.143.22.160                   # K8s Ingress 2
        - 16.143.22.162                   # K8s CP Worker 2
        - 16.143.22.164                   # K8s Worker 2
    #  status:   

  # Uncomment the folowing line to force the add_k8s_hosts playbook to include hosts in the repair_pool
  #force_repair: true

  istio:
    istio_home: "/var/lib/awx/istio-1.9.0"

  openebs:
    manifest_file: "openebs-operator-HA-1.9.0.yaml"
    storage_class_file: "openebs_create_storageclass.yaml"
    persistant_volume_claim_file: "openebs_create_pvc.yaml"

  tenant:
    name: Spark
    description: K8s cluster for Spark
    # Name of the K8s cluster this tenant is to be associated with
    cluster_name: test
    # Set the namespace name to be created or adopted. If not adopting an existing namespace and no namespace name is given, a unique name will be generated
    specified_namespace_name: spark
    # Flag to determine if associated namespace and all its contents should be deleted when the tenant is deleted. Defaults to true
    #is_namespace_owner: true
    # Flag to enable service mesh. This MUST be set to true for Splunk 2.0
    #enable_service_mesh_flag: true
    # Flag to map K8s servceis to HPE CP Gateway. This MUST be set to true for Splunk 2.0
    map_services_to_gateway_flag: true
    # Flag to adopt existing namespace fo the tenant. If set to true, a namespace name MUST be provided. If false, a namespace will be created
    #adopt_existing_namespace_flag: false
    # The following variables set the resource quotas for the new tenant. If not defined, resources will not be limited.
    # NOTE: for Splunk 2.0 no resource quotas will be set.
    quotas: 
      # cores: 64
      # Set the ephemeral disk space limit in GB
      # disk: 512
      # Set the persistent disk space limit in GB. Must be an integer > 20
      # persistent: 1024
      # gpus: 0
      # Set the maximum memory limit in GB
      # memory: 1024

    # NOTE: this variable is only used by the delete_k8s_tenant playbook
    target_tenant_name: test

  #*** UPDATE file locations as needed
  tools:
    kubeconfig_dir: "/var/lib/awx/.kube"
    kubeconfig_file: /tmp/kubeconfig_test.conf
    kubeconfig_context: CICL-splunk-admin
    kubectl_cli_version: v1.19.5

ecp_notification:
  #  snmp:
  #    server: "10.9.7.232"
  #    community: "public"
  #    engine: "Engine"
  #    user: "Username"
  #    authpassword: "Authpassword"
  #    authprotocol: "MD5"          # SHA, MD5
  #    privilege: "authPriv"        # authPriv, authNoPriv, noAuthNoPriv
  #    privprotocol: "AES"          # AES, DES
  #    privpassword: "privpassword"
  #  smtp:
  #    email: "test@mail.com"
  #    server: "smtp@mail.com"
  #    user: "username"
  #    password: "password"

