platform:
#  bin_file: "hpe-cp-rhel-release-5.1-1884.bin"
#  bin_dir: "/var/lib/awx/projects/_15__wf_lab92342_pm/playbooks/roles/platform/files/"
#  bin_url: "https://bluedata-releases.s3.amazonaws.com/GLDrops/hpe-cp-rhel-release-5.1-2055.bin"
  bin_url: "http://16.143.20.46:8080/rock/golden/GLDrops/hpe-cp-rhel-release-5.1-2055.bin"
#  bin_url: "http://16.143.20.46:8080/rock/golden/haathi-5.1-GA/3011/3011/hpe-cp-rhel-release-5.1-3011.bin"
#  controller: 16.0.8.228
  controller: mip-bdcs-vm94.mip.storage.hpecorp.net
# When running a one-off test, don't forget to just use one gateway
# and set gateway_set_hostname to the FQDN of that gateway
# Also, adjust the shadow_host_id and arbiter_host_id values below
  gateways:
    - mip-bdcs-vm95.mip.storage.hpecorp.net
 #   - mip-bdcs-vm96.mip.storage.hpecorp.net
  ha_workers: 
#    - mip-bdcs-vm21.mip.storage.hpecorp.net  # .146 - disk error issue
    - mip-bdcs-vm97.mip.storage.hpecorp.net
    - mip-bdcs-vm98.mip.storage.hpecorp.net
  k8s_workers:
#    - mip-bdcs-vm22.mip.storage.hpecorp.net  # temp arbiter due to .146 issues
    - mip-bdcs-vm99.mip.storage.hpecorp.net
    - mip-bdcs-vm37.mip.storage.hpecorp.net
    - mip-bdcs-vm38.mip.storage.hpecorp.net
    - mip-bdcs-vm39.mip.storage.hpecorp.net
  install_type: non-agent-based-install
  lockdown_reason: Configure HA
  name: CICL
  no_tenant_isolation: 'false'
  precheck_file: "bluedata-prechecks-epic-entdoc-5.0.bin"
  proxy: http://web-proxy.corp.hpecorp.net:8080
  type: onprem
  tools_dir: "/root/tools"         # place to store the kits like kubectl
  
gateway:
  gateway_set_hostname: mip-bdcs-vm95.mip.storage.hpecorp.net
#  gateway_set_hostname: hpecp-lab-gw.mip.storage.hpecorp.net

# Set the hosts IDs below to 3 and 4 if one gateway is configured, 4 and 5 if two gateways are configured
shadow_host_id: 3
arbiter_host_id: 4

airgap:
  container_repo_url: 16.143.22.165:5000
#  container_repo_username:
#  container_repo_password:
#  container_repo_secure_flag:
#  container_repo_cert:
  yum_repo_url: http://16.143.20.46:8080/scratch/qa/repos/base/
#  yum_repo_gpg_key:
    #yum_rpm_gpg_key:

controller:
  bd_domain: cicuat
  bd_prefix: wfecp
  int_start_ip: 172.20.0.2
  int_end_ip: 172.20.255.254
  int_gw_ip: 172.20.0.1
  int_nw_mask: 16

credentials:
  # Default installation accounts
  site_admin_id: admin
  site_admin_password: admin123
  api_session_id: /api/v1/session/7d8f3dde-0d36-40f0-a978-7c4d9d1e3fef
  ssh:
#    access_type: ssh_key_access
    access_type: password_access
    keypair_file: /home/root/.ssh/caas_rsa
    keypair_name: caas_rsa
    key_passphrase: 
    username: root
    usergroup: root
    password: admin123
#    password: ''  # Required to be set if using ssh?

disks:
  # Designated as node_disks for Controller and EPIC Worker hosts
  ephemeral_disks: 
    - /dev/sdb
#    - /dev/nvme0n1
  no_tenant_storage: 'true' 
  persistent_disks: ''

k8s_tools:
  kubeconfig_dir: "/var/lib/awx/.kube"
  kubeconfig_file: /tmp/kubeconfig_splunk.conf
  kubeconfig_context: CICL-splunk-admin
  kubectl_cli_version: v1.17.5

k8s_istio:
  istio_home: "/var/lib/awx/istio-1.6.5"

k8s_openebs:
  manifest_file: "openebs-operator-HA-1.9.0.yaml"
  storage_class_file: "openebs_create_storageclass.yaml"
  persistant_volume_claim_file: "openebs_create_pvc.yaml"

k8s_cluster:
  name: splunk
  description: K8s cluster for Splunk 2.0
  k8s_version: 1.17.5
# If no host IPs are specified below, the cluster will be created using all K8s hosts in inventory
  master_nodes:
    - 16.143.22.159
  ingress_nodes:
    - 16.143.22.160
  # Be sure to include these nodes in the list of worker_nodes
  cp_nodes:
    - 16.143.22.160
  worker_nodes:
    - 16.143.22.16
  id: 0
  pod_network_range: "10.192.0.0/12"
  service_network_range: "10.96.0.0/12"
  cert_data:
    root_ca_cert: 
    root_ca_key:
    front_proxy_ca_cert:
    front_proxy_ca_key:
    etcd_ca_cert:
    etcd_ca_key:
  ext_identity_server:
    type: LDAP
    ip: 
    port: 0
    auth_service_location_host:
    auth_service_location_port: 0
    timeout_ms: 1000
    reorder_after_failover: 'false'
    user_attribute:
    bind_type: search_bind
    base_dn:
    security_protocol: none
    cacert_filename:
    nt_domain:
    bind_dn:
    bind_pwd:
    verify_peer: 'true'
  external_groups: cn=Common_Name,ou=Organizational_Unit,dc=Domain_Component

k8s_tenant:
  name: splunk
  description: K8s cluster for Splunk
  # Name of the K8s cluster this tenant is to be associated with
  cluster_name: splunk
  # Set the namespace name to be created or adopted. If not adopting an existing namespace and no namespace name is given, a unique name will be generated
  specified_namespace_name: splunk
  # Flag to determine if associated namespace and all its contents should be deleted when the tenant is deleted. Defaults to true
  #is_namespace_owner: true
  # Flag to enable service mesh. This MUST be set to true for Splunk 2.0
  #enable_service_mesh_flag: true
  # Flag to map K8s servceis to HPE CP Gateway. This MUST be set to true for Splunk 2.0
  #map_services_to_gateway_flag: true
  # Flag to adopt existing namespace fo the tenant. If set to true, a namespace name MUST be provided. If false, a namespace will be created
  #adopt_existing_namespace_flag: false
  # The following variables set the resource quotas for the new tenant. If not defined, resources will not be limited.
  # NOTE: for Splunk 2.0 no resource quotas will be set.
  quotas: null
    # cores: 64
    # Set the ephemeral disk space limit in GB
    # disk: 512
    # Set the persistent disk space limit in GB. Must be an integer > 20
    # persistent: 1024
    # gpus: 0
    # Set the maximum memory limit in GB
    # memory: 1024
